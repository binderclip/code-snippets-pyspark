{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "001_logs_stats_map_reduce_filter_count.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/binderclip/code-snippets-pyspark/blob/master/prjs/001_logs_stats_map_reduce_filter_count.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V7NnTcFiNS0",
        "colab_type": "text"
      },
      "source": [
        "初始化环境"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZGtmuAkiLCv",
        "colab_type": "code",
        "outputId": "91b953d6-195f-4e97-9799-7f7e0de4adfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!pip install py4j"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting py4j\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/de/2d314a921ef4c20b283e1de94e0780273678caac901564df06b948e4ba9b/py4j-0.10.8.1-py2.py3-none-any.whl (196kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 12.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 92kB 3.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: py4j\n",
            "Successfully installed py4j-0.10.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsaq8EpaiTvE",
        "colab_type": "text"
      },
      "source": [
        "设置环境变量"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA5opkLmiZj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXj6eWwvibvc",
        "colab_type": "text"
      },
      "source": [
        "创建 spark session 和 spark context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bsd_TjeyimiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init(\"spark-2.4.4-bin-hadoop2.7\")# SPARK_HOME\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP-sRjtKi0j4",
        "colab_type": "text"
      },
      "source": [
        "spark context word count demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZoOoXw7i3F3",
        "colab_type": "code",
        "outputId": "b8563e19-d11f-4451-bd91-9166c5de9e65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# 每一行 log map 出来一个统计数据，再通过 reduce 来把结果的值合并起来\n",
        "# 先用一种比较挫的方式实现\n",
        "\n",
        "def get_count_abc_teamplate():\n",
        "  return {\n",
        "      \"a_0\": {\n",
        "          \"b_0\": 0,\n",
        "          \"b_1\": 0,\n",
        "          \"c_0\": 0,\n",
        "          \"c_1\": 0,\n",
        "      },\n",
        "      \"a_1\": {\n",
        "          \"b_0\": 0,\n",
        "          \"b_1\": 0,\n",
        "          \"c_0\": 0,\n",
        "          \"c_1\": 0,\n",
        "      },\n",
        "  }\n",
        "\n",
        "def count_abc(s):\n",
        "  d = get_count_abc_teamplate()\n",
        "  a = 0 if \"a=0\" in s else 1\n",
        "  b = 0 if \"b=0\" in s else 1\n",
        "  c = 0 if \"c=0\" in s else 1\n",
        "  if a:\n",
        "    if b:\n",
        "      d[\"a_1\"][\"b_1\"] = 1\n",
        "    else:\n",
        "      d[\"a_1\"][\"b_0\"] = 1\n",
        "    if c:\n",
        "      d[\"a_1\"][\"c_1\"] = 1\n",
        "    else:\n",
        "      d[\"a_1\"][\"c_0\"] = 1\n",
        "  else:\n",
        "    if b:\n",
        "      d[\"a_0\"][\"b_1\"] = 1\n",
        "    else:\n",
        "      d[\"a_0\"][\"b_0\"] = 1\n",
        "    if c:\n",
        "      d[\"a_0\"][\"c_1\"] = 1\n",
        "    else:\n",
        "      d[\"a_0\"][\"c_0\"] = 1\n",
        "  return d\n",
        "\n",
        "def count_abc_sum(d1, d2):\n",
        "  d3 = get_count_abc_teamplate()\n",
        "  d3[\"a_1\"][\"b_1\"] = d1[\"a_1\"][\"b_1\"] + d2[\"a_1\"][\"b_1\"]\n",
        "  d3[\"a_1\"][\"b_0\"] = d1[\"a_1\"][\"b_0\"] + d2[\"a_1\"][\"b_0\"]\n",
        "  d3[\"a_1\"][\"c_1\"] = d1[\"a_1\"][\"c_1\"] + d2[\"a_1\"][\"c_1\"]\n",
        "  d3[\"a_1\"][\"c_0\"] = d1[\"a_1\"][\"c_0\"] + d2[\"a_1\"][\"c_0\"]\n",
        "  d3[\"a_0\"][\"b_1\"] = d1[\"a_0\"][\"b_1\"] + d2[\"a_0\"][\"b_1\"]\n",
        "  d3[\"a_0\"][\"b_0\"] = d1[\"a_0\"][\"b_0\"] + d2[\"a_0\"][\"b_0\"]\n",
        "  d3[\"a_0\"][\"c_1\"] = d1[\"a_0\"][\"c_1\"] + d2[\"a_0\"][\"c_1\"]\n",
        "  d3[\"a_0\"][\"c_0\"] = d1[\"a_0\"][\"c_0\"] + d2[\"a_0\"][\"c_0\"]\n",
        "  return d3\n",
        "\n",
        "logs_rdd = sc.parallelize([\n",
        "                           \"a=1, b=1, c=0\",\n",
        "                           \"a=1, b=0, c=0\",\n",
        "                           \"a=0, b=0, c=0\",\n",
        "                           ])\n",
        "counts = logs_rdd.map(lambda s: count_abc(s)) \\\n",
        "    .reduce(lambda d1, d2: count_abc_sum(d1, d2))\n",
        "print(counts)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a_0': {'b_0': 1, 'b_1': 0, 'c_0': 1, 'c_1': 0}, 'a_1': {'b_0': 1, 'b_1': 1, 'c_0': 2, 'c_1': 0}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmJ9dAfjpU2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 每一行 log map 出来一个统计数据，再通过 reduce 来把结果的值合并起来\n",
        "# 先用一种比较挫的方式实现\n",
        "\n",
        "def get_count_abc_teamplate():\n",
        "  return {\n",
        "      \"a_0\": {\n",
        "          \"b_0\": 0,\n",
        "          \"b_1\": 0,\n",
        "          \"c_0\": 0,\n",
        "          \"c_1\": 0,\n",
        "      },\n",
        "      \"a_1\": {\n",
        "          \"b_0\": 0,\n",
        "          \"b_1\": 0,\n",
        "          \"c_0\": 0,\n",
        "          \"c_1\": 0,\n",
        "      },\n",
        "  }\n",
        "\n",
        "def count_abc(s):\n",
        "  d = get_count_abc_teamplate()\n",
        "  a = 0 if \"a=0\" in s else 1\n",
        "  b = 0 if \"b=0\" in s else 1\n",
        "  c = 0 if \"c=0\" in s else 1\n",
        "  if a:\n",
        "    if b:\n",
        "      d[\"a_1\"][\"b_1\"] = 1\n",
        "    else:\n",
        "      d[\"a_1\"][\"b_0\"] = 1\n",
        "    if c:\n",
        "      d[\"a_1\"][\"c_1\"] = 1\n",
        "    else:\n",
        "      d[\"a_1\"][\"c_0\"] = 1\n",
        "  else:\n",
        "    if b:\n",
        "      d[\"a_0\"][\"b_1\"] = 1\n",
        "    else:\n",
        "      d[\"a_0\"][\"b_0\"] = 1\n",
        "    if c:\n",
        "      d[\"a_0\"][\"c_1\"] = 1\n",
        "    else:\n",
        "      d[\"a_0\"][\"c_0\"] = 1\n",
        "  return d\n",
        "\n",
        "def count_abc_sum(d1, d2):\n",
        "  d3 = get_count_abc_teamplate()\n",
        "  d3[\"a_1\"][\"b_1\"] = d1[\"a_1\"][\"b_1\"] + d2[\"a_1\"][\"b_1\"]\n",
        "  d3[\"a_1\"][\"b_0\"] = d1[\"a_1\"][\"b_0\"] + d2[\"a_1\"][\"b_0\"]\n",
        "  d3[\"a_1\"][\"c_1\"] = d1[\"a_1\"][\"c_1\"] + d2[\"a_1\"][\"c_1\"]\n",
        "  d3[\"a_1\"][\"c_0\"] = d1[\"a_1\"][\"c_0\"] + d2[\"a_1\"][\"c_0\"]\n",
        "  d3[\"a_0\"][\"b_1\"] = d1[\"a_0\"][\"b_1\"] + d2[\"a_0\"][\"b_1\"]\n",
        "  d3[\"a_0\"][\"b_0\"] = d1[\"a_0\"][\"b_0\"] + d2[\"a_0\"][\"b_0\"]\n",
        "  d3[\"a_0\"][\"c_1\"] = d1[\"a_0\"][\"c_1\"] + d2[\"a_0\"][\"c_1\"]\n",
        "  d3[\"a_0\"][\"c_0\"] = d1[\"a_0\"][\"c_0\"] + d2[\"a_0\"][\"c_0\"]\n",
        "  return d3\n",
        "\n",
        "logs_rdd = sc.parallelize([\n",
        "                           \"a=1, b=1, c=0\",\n",
        "                           \"a=1, b=0, c=0\",\n",
        "                           \"a=0, b=0, c=0\",\n",
        "                           ])\n",
        "counts = logs_rdd.map(lambda s: count_abc(s)) \\\n",
        "    .reduce(lambda d1, d2: count_abc_sum(d1, d2))\n",
        "print(counts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBVamuwJryVO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f796727d-0a51-49bc-8c6e-9d97b2219869"
      },
      "source": [
        "# 每一行 log map 先预处理数据，然后分别 filter count，得到不同 case 的数量\n",
        "\n",
        "def parse_log(s):\n",
        "  a = 0 if \"a=0\" in s else 1\n",
        "  b = 0 if \"b=0\" in s else 1\n",
        "  c = 0 if \"c=0\" in s else 1\n",
        "  return a, b, c\n",
        "\n",
        "logs_rdd = sc.parallelize([\n",
        "                           \"a=1, b=1, c=0\",\n",
        "                           \"a=1, b=0, c=0\",\n",
        "                           \"a=0, b=0, c=0\",\n",
        "                           ])\n",
        "parsed_logs_rdd = logs_rdd.map(parse_log)\n",
        "count_a_0_b_0 = parsed_logs_rdd.filter(lambda t: t[0] == 0 and t[1] == 0).count()\n",
        "count_a_1_c_0 = parsed_logs_rdd.filter(lambda t: t[0] == 1 and t[2] == 0).count()\n",
        "print(\"a=0, b=0:\", count_a_0_b_0)\n",
        "print(\"a=1, c=0:\", count_a_1_c_0)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a=0, b=0: 1\n",
            "a=1, c=0: 2\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}